{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "VACCINe_walkthrough.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SajjadAbedian/VACCINe/blob/master/VACCINe_walkthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf8GfHQ3nURV"
      },
      "source": [
        "# VACCINe - Walkthrough\n",
        "In this tutorial, we will go over the steps to access, manipulate, and join disparate Social Determinants of Health datasets.\n",
        "\n",
        "## Predefined functions\n",
        "First, we need to create a series of functions that we will use throughout this walkthrough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKmJCWsPnURY"
      },
      "source": [
        "### geo_to_fips\n",
        "`geo_to_fips(dataframe, latitude, longitude)`\n",
        "\n",
        "We use this function to convert pairs of latitudes and longitudes to 15-digits FIPS codes. \n",
        "\n",
        "#### Input\n",
        "* First argument (`dataframe`) is a data frame containing at least a latitude and longitude columns\n",
        "* Second argument (`latitude`) must be the name of the column which stores the latitude addresses (as a string)\n",
        "* Third argument (`longitude`) must be the name of the column that stores the longitude addresses (as a string)\n",
        "* **NOTE:** For larger datasets (more than 10000 rows), please be sure to break up the dataset into smaller subsets before using this function. This function uses the Federal Communications Commission's API to convert latitude and longitude pairs to FIPS codes.\n",
        "\n",
        "#### Output\n",
        "* The output is a new pandas dataframe with an additional column containing the 15-digits FIPS codes\n",
        "    * NOTE: you should save the result of the function in a new variable\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGenT_2wnURZ"
      },
      "source": [
        "def geo_to_fips(dataframe, latitude, longitude):\n",
        "    fips = []\n",
        "    for index, value in enumerate(dataframe.iloc[:,0]):\n",
        "        url = \"https://geo.fcc.gov/api/census/block/find?latitude=\" + str(dataframe[latitude][index]) + \"&longitude=\"+ str(dataframe[longitude][index]) + \"&format=json\"\n",
        "        obj = json.load(urlopen(url))\n",
        "        fips.append(obj['Block']['FIPS'])\n",
        "    dataframe[\"FIPS\"] = fips\n",
        "    print(\"FIPS code converted\")\n",
        "    return  dataframe"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVfoeGC8nURc"
      },
      "source": [
        "### fips_check\n",
        "`fips_check(dataframe, col)`\n",
        "\n",
        "We use this function to identify the length of the FIPS code in any dataset. This function displays the length of the largest FIPS code in a column.\n",
        "\n",
        "#### Input\n",
        "* first argument (`dataframe`) is the name of your dataset\n",
        "* second argument (`col`) is the name of the column which holds the FIPS codes\n",
        "    * NOTE: col must be a string variable\n",
        "\n",
        "#### Output\n",
        "* Returns the length of the largest FIPS code in the `col` column of your dataset `dataframe`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jS9DNLdnURc"
      },
      "source": [
        "def fips_check(dataframe, col):\n",
        "    tract = []\n",
        "    for n in dataframe[col]:\n",
        "        tract.append(len(str(n)))\n",
        "    return max(tract)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xBqs5JQnURe"
      },
      "source": [
        "### fips_11 & fips_12\n",
        "`fips_11(dataframe, column_list = [])` & `fips_12(dataframe, column_list = [])`\n",
        "\n",
        "We will use these two functions frequently throughout this whole walkthrough to convert the 15-digits FIPS codes to 11 or 12-digits FIPS codes. These functions also serve as a slicer, to reduce the main datasets to only the necessary columns.\n",
        "\n",
        "#### Input: \n",
        "* First argument (`dataframe`) is the variable containing the original dataset \n",
        "* Second argument (`column_list = []`) is a LIST variable that you can name all the columns you would like to include in the final dataset. You can list the columns in any order you would like, as long as the first item is the column with the FIPS code.\n",
        "    * NOTE: column_list is a list variable containing all column names as strings\n",
        "    * NOTE: the first column in your original dataset should be the column containing the geographical unit\n",
        "\n",
        "#### Output:\n",
        "* The output is a pandas data frame with the desired columns and 11 (or 12) digits FIPS codes.\n",
        "* It also includes a column with the geographical unit of interest renamed to _FIPS_.\n",
        "* The result should be saved in a separate variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6j3KfzMnURf"
      },
      "source": [
        "def fips_11(dataframe, column_list = []):\n",
        "    df_new = dataframe.loc[:, column_list]\n",
        "    df_new[column_list[0]] = df_new[column_list[0]].astype(str)\n",
        "    df_new['CT'] = df_new[column_list[0]].str[0:11].astype(str)\n",
        "    return df_new\n",
        "\n",
        "\n",
        "\n",
        "def fips_12(dataframe, column_list = []):\n",
        "    df_new = dataframe.loc[:, column_list]\n",
        "    df_new[column_list[0]] = df_new[column_list[0]].astype(str)\n",
        "    df_new['BG'] = df_new[column_list[0]].str[0:12].astype(str)\n",
        "    return df_new"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw6DO3ZenURi"
      },
      "source": [
        "### join_to_vaccine\n",
        "`join_to_vaccine(V, X)`\n",
        "\n",
        "#### Input\n",
        "* `V` is the main SDH dataframe\n",
        "* `X` is the processed SDH source data you would like to join to the main SDH dataframe\n",
        "\n",
        "#### Output\n",
        "* The output is a pandas data frame\n",
        "* The outout contains the main SDH data frame used as the input of this function (`V`) alongwith the additional columns from the processed SDH source data (`X`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S51Jb_oZnURj"
      },
      "source": [
        "def join_to_vaccine(V, V_index, X, X_index):\n",
        "    V[V_index] = V[V_index].astype(str)\n",
        "    X[X_index] = X[X_index].astype(str)\n",
        "    return V.join(X.set_index(X_index), on=V_index)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BafgN2TnURn"
      },
      "source": [
        "## Importing modules\n",
        "Modules below are all essential. Please use the command below to install any module. \n",
        "\n",
        "\n",
        "from census import Census\n",
        "from us import states\n",
        "import pandas as pd \n",
        "import json\n",
        "import requests\n",
        "from urllib.request import urlopen\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "import zipfile\n",
        "import requests, zipfile, io\n",
        "from datetime import date "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bBIOkdWXX7R",
        "outputId": "acbec3e3-ad70-4e67-d10d-675f6d181ab0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install census us"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting census\n",
            "  Downloading https://files.pythonhosted.org/packages/63/a4/3f445a8a8ba9003f01dab31afb2baff522a786eb860c792baac819fdccc6/census-0.8.15-py2.py3-none-any.whl\n",
            "Collecting us\n",
            "  Downloading https://files.pythonhosted.org/packages/88/04/04323aefa1871de30286d3decae7706481c73bd428cf0c08e158bfa259a6/us-2.0.2.tar.gz\n",
            "Requirement already satisfied: requests>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from census) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from census) (0.16.0)\n",
            "Collecting jellyfish==0.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/3f/60ac86fb43dfbf976768e80674b5538e535f6eca5aa7806cf2fdfd63550f/jellyfish-0.6.1.tar.gz (132kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=1.1.0->census) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=1.1.0->census) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=1.1.0->census) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=1.1.0->census) (3.0.4)\n",
            "Building wheels for collected packages: us, jellyfish\n",
            "  Building wheel for us (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for us: filename=us-2.0.2-cp36-none-any.whl size=11928 sha256=466e5ff079ce181616b15ff569e30b9b427af2935eb131cdc234ea772c11db6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/16/45/6453383ffa495670f0f6b80a3e697a9771d98cfbaf8b451e73\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.6.1-cp36-cp36m-linux_x86_64.whl size=74760 sha256=c73f79a28c4345d94d2b111b9d65bce5ad0c7931653398c23591fd1585446fdd\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/6f/33/92bb9a4b4562a60ba6a80cedbab8907e48bc7a8b1f369ea0ae\n",
            "Successfully built us jellyfish\n",
            "Installing collected packages: census, jellyfish, us\n",
            "Successfully installed census-0.8.15 jellyfish-0.6.1 us-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXOMgXzBnURn"
      },
      "source": [
        "from census import Census\n",
        "from us import states\n",
        "import pandas as pd \n",
        "import json\n",
        "import requests\n",
        "from urllib.request import urlopen\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "import zipfile\n",
        "import requests, zipfile, io\n",
        "from datetime import date \n",
        "from google.colab import files"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0FptUmJnURr"
      },
      "source": [
        "## American Community Survey (ACS)\n",
        "We will use ACS's API to access its data.\n",
        "\n",
        "It is important to note, the main SDH dataset should start with a column that includes all the geographical units of interest. We chose to start constructing VACCINe by incorporating ACS data since ACS provides data for **ALL** of FIPS codes. \n",
        "\n",
        "If you are not using ACS, you may obtain the list of geographical units of your area in a separate column in a dataframe as your starting point. Then you would be able to join additional datasets to this column. You can easily use ACS's API to generate the FIPS column.\n",
        "\n",
        "### API setup\n",
        "In order to use the API, you must acquire an API key. See address below:\n",
        "https://api.census.gov/data/key_signup.html\n",
        "\n",
        "To read the detailed API documentation, please visit the website below:\n",
        "https://github.com/datamade/census \n",
        "\n",
        "Demystifying the Census API webinar: https://www.census.gov/data/academy/webinars/2020/demystifying-the-census-api.html?utm_campaign=20200722mscacs2ccstars&utm_medium=email&utm_source=govdelivery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c5G6L1ynURs"
      },
      "source": [
        "c = Census(\"f42ca37e5880674136047af378c64f8e12de51f4\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lClwK8dxnURv"
      },
      "source": [
        "### Organizing the variables of interest\n",
        "We create a dictionary from the variables we want to include in the dataset along with a human-readable name for each. Note that you can name these variables based on your preference. For example, we know `B27010_023E` corresponds to the number of 18 to 34 years olds who only have Medicaid. We chose to name this variable `18_34y__MedcaidOnly`, however, you have the flexibility to name this based on your liking. \n",
        "\n",
        "This step will be crucial to \"decode\" ambiguous variable names in our dataset and rename the column headers.\n",
        "\n",
        "To obtain a complete list of available variables please visit: https://www.census.gov/programs-surveys/acs/guidance/which-data-tool/table-ids-explained.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Taa8Zx-5nURv"
      },
      "source": [
        "dataHeadDict = {\n",
        "    \"NAME\" : \"Description\",\n",
        "    \"B99051_001E\":\"fbTotal\",\n",
        "    \"B99051_005E\" : \"foreignBorn\",\n",
        "    \n",
        "    \"B27010_033E\" : \"18_34y__NOinsurance\", \n",
        "    \"B27010_050E\" : \"35_64y__NOinsurance\",\n",
        "    \"B27010_018E\" : \"18_34y__total\",\n",
        "    \"B27010_034E\": \"35_64y__total\", \n",
        "    \"B27010_023E\" : \"18_34y__MedcaidOnly\", \n",
        "    \"B27010_039E\" : \"35_64y__MedcaidOnly\",\n",
        "    \"B27010_029E\" : \"18_34y__Medicare/Medicaid\",\n",
        "    \"B27010_046E\" : \"35_64y__Medicare/Medicaid\", \n",
        "    \n",
        "    \"B15003_001E\": \"totalPop_b\",\n",
        "    \"B15003_017E\" : \"high_school_diploma\", \n",
        "    \"B15003_018E\" : \"GED\", \n",
        "    \"B15003_002E\" : \"no_school\", \n",
        "    \"B15003_003E\" : \"nursery_school\", \n",
        "    \"B15003_004E\" : \"kindergarten\", \n",
        "    \"B15003_005E\" : \"grade1_school\", \n",
        "    \"B15003_006E\" : \"grade2_school\", \n",
        "    \"B15003_007E\" : \"grade3_school\",\n",
        "    \"B15003_008E\" : \"grade4_school\",\n",
        "    \"B15003_009E\" : \"grade5_school\",\n",
        "    \"B15003_010E\" : \"grade6_school\",\n",
        "    \"B15003_011E\" : \"grade7_school\",\n",
        "    \"B15003_012E\" : \"grade8_school\",\n",
        "    \"B15003_013E\" : \"grade9_school\",\n",
        "    \"B15003_014E\" : \"grade10_school\",\n",
        "    \"B15003_015E\" : \"grade11_school\",\n",
        "    \"B15003_016E\" : \"grade12_school_no_diploma\", \n",
        "    \n",
        "    \"B23025_005E\" : \"unemployed_labor_force_16up\", \n",
        "    \"B23025_002E\" : \"labor_force_16up\", \n",
        "    \n",
        "    \"B19113_001E\" : \"medianHouseIncome\", \n",
        "    \n",
        "    \"C17002_001E\" : \"totalPop_a\", \n",
        "    \"C17002_002E\" : \"estimateUnder_.50\",\n",
        "    \"C17002_003E\" : \"estimate_.50_.99\",\n",
        "    \n",
        "    \"B19083_001E\" : \"GINI\",\n",
        "    \"B01003_001E\" : \"totalPop\"\n",
        "}\n",
        "\n",
        "censusCodes = list(dataHeadDict.keys())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsdynFvcnURx"
      },
      "source": [
        "All variables are taken from ACS 5 year estimate 2016 as listed above. `censusCodes` variable contains only the keys in `dataHeadDict` (i.e. the coded variable names)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w2UPKjTnURy"
      },
      "source": [
        "#ACS 5 year estimate 2016.\n",
        "acs5DataCT = c.acs5.get(censusCodes, geo = {'for': 'tract:*', 'in':'state:{}'.format(states.NY.fips)}, year =2016)\n",
        "dfCT = pd.DataFrame(data = acs5DataCT)\n",
        "\n",
        "#create census tract fips code \n",
        "dfCT['Census_Tract'] = dfCT['state'].map(str) + dfCT['county'].map(str) + dfCT['tract'].map(str)\n",
        "\n",
        "# include only NYC in dfCT\n",
        "dfCT = dfCT[(dfCT.county == '005')| (dfCT.county =='047')| (dfCT.county == '061') |(dfCT.county == '081')| (dfCT.county == '085')]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jtugbYmnUR0"
      },
      "source": [
        "You are not only limited to variables you can pull from ACS dataset. You can combine them when appropriate to derive new variables. Below we are calculate the percentage for some of these variables based on the total population of each geographical area."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxiRvAAEnUR0"
      },
      "source": [
        "dfCT['foriegnBornPercent'] = dfCT.B99051_005E / dfCT.B99051_001E *100\n",
        "dfCT['woInsurance18_64Percent'] = (dfCT.B27010_033E + dfCT.B27010_050E) / (dfCT.B27010_018E + dfCT.B27010_034E)  *100\n",
        "dfCT['medicaid18_64Percent'] = (dfCT.B27010_023E + dfCT.B27010_039E) / (dfCT.B27010_018E + dfCT.B27010_034E)  *100\n",
        "dfCT['medicaid_Medicare18_64Percent'] = (dfCT.B27010_029E + dfCT.B27010_046E) / (dfCT.B27010_018E + dfCT.B27010_034E)  *100\n",
        "dfCT['no_school_to_12_grade_no_diplomaPercent'] = (dfCT.B15003_002E + dfCT.B15003_003E + dfCT.B15003_004E +  dfCT.B15003_005E+ \\\n",
        "                                                dfCT.B15003_006E+  dfCT.B15003_007E+  dfCT.B15003_008E+  dfCT.B15003_009E+  \\\n",
        "                                               dfCT.B15003_010E+  dfCT.B15003_011E+  dfCT.B15003_012E+  dfCT.B15003_013E+  \\\n",
        "                                               dfCT.B15003_014E+  dfCT.B15003_015E+  dfCT.B15003_016E) / dfCT.B15003_001E  *100\n",
        "dfCT['highSchool_GEDPercent'] = (dfCT.B15003_017E + dfCT.B15003_018E) / dfCT.B15003_001E  *100\n",
        "dfCT['unemployedPercent'] = dfCT.B23025_005E / dfCT.B23025_002E *100\n",
        "\n",
        "dfCT['medianHouseIncome'] = dfCT.B19113_001E \n",
        "dfCT['belowPovertyPercent'] = (dfCT.C17002_002E + dfCT.C17002_003E) / dfCT.C17002_001E *100 \n",
        "dfCT['GINI'] = dfCT['B19083_001E'].apply(pd.to_numeric)\n",
        "dfCT['totalPopulation'] = dfCT.B01003_001E\n",
        "\n",
        "#remove original raw data \n",
        "dfCT = dfCT.drop(censusCodes, axis =1)\n",
        "dfCT = dfCT.drop(['county','state','tract'], axis =1)\n",
        "dfCT = dfCT.reset_index(drop = True)\n",
        "dfCT.rename(columns={dfCT.columns[0]:'FIPS'}, inplace=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukKDabNsnUR4"
      },
      "source": [
        "## Air quality - Environmental Protection Agency\n",
        "This data set has several useful variables. However, we are only using _Total Respiratory (hazard quotient)_.\n",
        "\n",
        "### Reading the dataset\n",
        "We access the dataset directly from EPA's website by reading the URL in pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_92W95N5nUR5"
      },
      "source": [
        "url_epa = \"https://www.epa.gov/sites/production/files/2018-08/nata2014v2_national_resphi_by_tract_poll.xlsx\"\n",
        "epa_df = pd.read_excel(url_epa, sheet_name=0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGB-HlX1nUR7"
      },
      "source": [
        "### Exploring the dataset\n",
        "It's a good practice to explore the dataset at hand by looking at the column names (or data dictionary if available), some sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAzzLHaqnUR7",
        "outputId": "cc927711-4ee5-4929-b83a-5458e3771e93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "epa_df.columns"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['State', 'EPA Region', 'County', 'FIPS', 'Tract', 'Population',\n",
              "       'Total Respiratory (hazard quotient)', '1,2-EPOXYBUTANE',\n",
              "       '1,3-DICHLOROPROPENE', '2-CHLOROACETOPHENONE',\n",
              "       '2,4-TOLUENE DIISOCYANATE', '4,4'-METHYLENEDIPHENYL DIISOCYANATE (MDI)',\n",
              "       'ACETALDEHYDE', 'ACROLEIN', 'ACRYLIC ACID', 'ACRYLONITRILE',\n",
              "       'ANTIMONY COMPOUNDS', 'BERYLLIUM COMPOUNDS',\n",
              "       'BIS(2-ETHYLHEXYL)PHTHALATE (DEHP)', 'CHROMIUM VI (HEXAVALENT)',\n",
              "       'CHLORINE', 'CHLOROPRENE', 'COBALT COMPOUNDS', 'DIESEL PM',\n",
              "       'DIETHANOLAMINE', 'EPICHLOROHYDRIN',\n",
              "       'ETHYLENE DIBROMIDE (DIBROMOETHANE)', 'ETHYLENE GLYCOL', 'FORMALDEHYDE',\n",
              "       'HEXACHLOROCYCLOPENTADIENE', 'HEXAMETHYLENE DIISOCYANATE',\n",
              "       'HYDROCHLORIC ACID (HYDROGEN CHLORIDE [GAS ONLY])', 'MALEIC ANHYDRIDE',\n",
              "       'METHYL BROMIDE (BROMOMETHANE)', 'METHYL ISOCYANATE',\n",
              "       'METHYL METHACRYLATE', 'METHYLENE CHLORIDE', 'NICKEL COMPOUNDS',\n",
              "       'NAPHTHALENE', 'NITROBENZENE', 'PHOSGENE', 'PHTHALIC ANHYDRIDE',\n",
              "       'PROPIONALDEHYDE', 'PROPYLENE DICHLORIDE (1,2-DICHLOROPROPANE)',\n",
              "       'PROPYLENE OXIDE', 'STYRENE OXIDE', 'TITANIUM TETRACHLORIDE',\n",
              "       'TRIETHYLAMINE', 'VINYL ACETATE', '1,4-DIOXANE'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELJtRRVmnUR9",
        "outputId": "ecce5cdd-f988-4e51-afd5-cf5a91111a56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "epa_df.sample(5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>State</th>\n",
              "      <th>EPA Region</th>\n",
              "      <th>County</th>\n",
              "      <th>FIPS</th>\n",
              "      <th>Tract</th>\n",
              "      <th>Population</th>\n",
              "      <th>Total Respiratory (hazard quotient)</th>\n",
              "      <th>1,2-EPOXYBUTANE</th>\n",
              "      <th>1,3-DICHLOROPROPENE</th>\n",
              "      <th>2-CHLOROACETOPHENONE</th>\n",
              "      <th>2,4-TOLUENE DIISOCYANATE</th>\n",
              "      <th>4,4'-METHYLENEDIPHENYL DIISOCYANATE (MDI)</th>\n",
              "      <th>ACETALDEHYDE</th>\n",
              "      <th>ACROLEIN</th>\n",
              "      <th>ACRYLIC ACID</th>\n",
              "      <th>ACRYLONITRILE</th>\n",
              "      <th>ANTIMONY COMPOUNDS</th>\n",
              "      <th>BERYLLIUM COMPOUNDS</th>\n",
              "      <th>BIS(2-ETHYLHEXYL)PHTHALATE (DEHP)</th>\n",
              "      <th>CHROMIUM VI (HEXAVALENT)</th>\n",
              "      <th>CHLORINE</th>\n",
              "      <th>CHLOROPRENE</th>\n",
              "      <th>COBALT COMPOUNDS</th>\n",
              "      <th>DIESEL PM</th>\n",
              "      <th>DIETHANOLAMINE</th>\n",
              "      <th>EPICHLOROHYDRIN</th>\n",
              "      <th>ETHYLENE DIBROMIDE (DIBROMOETHANE)</th>\n",
              "      <th>ETHYLENE GLYCOL</th>\n",
              "      <th>FORMALDEHYDE</th>\n",
              "      <th>HEXACHLOROCYCLOPENTADIENE</th>\n",
              "      <th>HEXAMETHYLENE DIISOCYANATE</th>\n",
              "      <th>HYDROCHLORIC ACID (HYDROGEN CHLORIDE [GAS ONLY])</th>\n",
              "      <th>MALEIC ANHYDRIDE</th>\n",
              "      <th>METHYL BROMIDE (BROMOMETHANE)</th>\n",
              "      <th>METHYL ISOCYANATE</th>\n",
              "      <th>METHYL METHACRYLATE</th>\n",
              "      <th>METHYLENE CHLORIDE</th>\n",
              "      <th>NICKEL COMPOUNDS</th>\n",
              "      <th>NAPHTHALENE</th>\n",
              "      <th>NITROBENZENE</th>\n",
              "      <th>PHOSGENE</th>\n",
              "      <th>PHTHALIC ANHYDRIDE</th>\n",
              "      <th>PROPIONALDEHYDE</th>\n",
              "      <th>PROPYLENE DICHLORIDE (1,2-DICHLOROPROPANE)</th>\n",
              "      <th>PROPYLENE OXIDE</th>\n",
              "      <th>STYRENE OXIDE</th>\n",
              "      <th>TITANIUM TETRACHLORIDE</th>\n",
              "      <th>TRIETHYLAMINE</th>\n",
              "      <th>VINYL ACETATE</th>\n",
              "      <th>1,4-DIOXANE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50501</th>\n",
              "      <td>NC</td>\n",
              "      <td>EPA Region 4</td>\n",
              "      <td>Greene</td>\n",
              "      <td>37079</td>\n",
              "      <td>37079950300</td>\n",
              "      <td>7380</td>\n",
              "      <td>0.378194</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.314477e-07</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.145036</td>\n",
              "      <td>0.049583</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>3.863480e-06</td>\n",
              "      <td>0.000090</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>2.672524e-03</td>\n",
              "      <td>3.777777e-09</td>\n",
              "      <td>5.730814e-06</td>\n",
              "      <td>0.017173</td>\n",
              "      <td>7.026457e-08</td>\n",
              "      <td>1.238167e-07</td>\n",
              "      <td>7.702619e-07</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.146364</td>\n",
              "      <td>9.353815e-09</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.004727</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005654</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.085805e-06</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>0.001281</td>\n",
              "      <td>0.003902</td>\n",
              "      <td>2.346053e-09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.125483e-09</td>\n",
              "      <td>0.000453</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>8.536462e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.662758e-05</td>\n",
              "      <td>1.620393e-07</td>\n",
              "      <td>2.712139e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40570</th>\n",
              "      <td>NE</td>\n",
              "      <td>EPA Region 7</td>\n",
              "      <td>Box Butte</td>\n",
              "      <td>31013</td>\n",
              "      <td>31013951300</td>\n",
              "      <td>4217</td>\n",
              "      <td>0.202593</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.060332</td>\n",
              "      <td>0.017991</td>\n",
              "      <td>0.000283</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>7.333967e-09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6.011369e-07</td>\n",
              "      <td>0.038615</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.071405</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004328</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005600</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.263476e-09</td>\n",
              "      <td>0.000776</td>\n",
              "      <td>0.000484</td>\n",
              "      <td>0.001961</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000602</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.187953e-07</td>\n",
              "      <td>2.542217e-08</td>\n",
              "      <td>2.867094e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6684</th>\n",
              "      <td>CA</td>\n",
              "      <td>EPA Region 9</td>\n",
              "      <td>Los Angeles</td>\n",
              "      <td>6037</td>\n",
              "      <td>6037552602</td>\n",
              "      <td>4191</td>\n",
              "      <td>0.610047</td>\n",
              "      <td>4.745295e-07</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.138650</td>\n",
              "      <td>0.102557</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.001411</td>\n",
              "      <td>1.284501e-04</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>6.697362e-02</td>\n",
              "      <td>6.353565e-08</td>\n",
              "      <td>5.866910e-05</td>\n",
              "      <td>0.068466</td>\n",
              "      <td>4.922368e-04</td>\n",
              "      <td>3.891234e-07</td>\n",
              "      <td>2.020861e-06</td>\n",
              "      <td>0.000639</td>\n",
              "      <td>0.173261</td>\n",
              "      <td>2.499069e-07</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>0.008356</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.007585</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.303484e-05</td>\n",
              "      <td>0.001720</td>\n",
              "      <td>0.013849</td>\n",
              "      <td>0.017461</td>\n",
              "      <td>6.250421e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.343278e-06</td>\n",
              "      <td>0.006980</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>2.486176e-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.974848e-05</td>\n",
              "      <td>1.256359e-06</td>\n",
              "      <td>3.285737e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3365</th>\n",
              "      <td>AR</td>\n",
              "      <td>EPA Region 6</td>\n",
              "      <td>Logan</td>\n",
              "      <td>5083</td>\n",
              "      <td>5083950400</td>\n",
              "      <td>3322</td>\n",
              "      <td>0.513697</td>\n",
              "      <td>9.997639e-07</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.178587</td>\n",
              "      <td>0.107243</td>\n",
              "      <td>0.001963</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>4.321343e-06</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>1.137324e-03</td>\n",
              "      <td>2.806570e-10</td>\n",
              "      <td>8.440664e-06</td>\n",
              "      <td>0.014010</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.662346e-09</td>\n",
              "      <td>5.873826e-06</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.192435</td>\n",
              "      <td>2.356805e-09</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004306</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005600</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.605536e-08</td>\n",
              "      <td>0.000310</td>\n",
              "      <td>0.000487</td>\n",
              "      <td>0.006968</td>\n",
              "      <td>5.900820e-10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>4.304266e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.884815e-06</td>\n",
              "      <td>3.763678e-08</td>\n",
              "      <td>4.225955e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73815</th>\n",
              "      <td>WV</td>\n",
              "      <td>EPA Region 3</td>\n",
              "      <td>McDowell</td>\n",
              "      <td>54047</td>\n",
              "      <td>54047954200</td>\n",
              "      <td>1886</td>\n",
              "      <td>0.348401</td>\n",
              "      <td>1.795590e-10</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>4.289603e-08</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.096432</td>\n",
              "      <td>0.093112</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>1.484295e-07</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>7.392868e-04</td>\n",
              "      <td>1.391100e-09</td>\n",
              "      <td>2.789137e-07</td>\n",
              "      <td>0.011937</td>\n",
              "      <td>2.781478e-08</td>\n",
              "      <td>2.123938e-09</td>\n",
              "      <td>5.544352e-06</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.124962</td>\n",
              "      <td>1.366800e-09</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.004306</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005600</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.243712e-08</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.000931</td>\n",
              "      <td>0.007486</td>\n",
              "      <td>3.429650e-10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000142</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>1.148456e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.558302e-06</td>\n",
              "      <td>2.572161e-08</td>\n",
              "      <td>3.267177e-08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      State    EPA Region  ... VINYL ACETATE   1,4-DIOXANE\n",
              "50501    NC  EPA Region 4  ...  1.620393e-07  2.712139e-07\n",
              "40570    NE  EPA Region 7  ...  2.542217e-08  2.867094e-08\n",
              "6684     CA  EPA Region 9  ...  1.256359e-06  3.285737e-07\n",
              "3365     AR  EPA Region 6  ...  3.763678e-08  4.225955e-08\n",
              "73815    WV  EPA Region 3  ...  2.572161e-08  3.267177e-08\n",
              "\n",
              "[5 rows x 50 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjFYQK4lnUR_"
      },
      "source": [
        "### Preprocessing \n",
        "We can now start manipulating the original dataset using the predefined functions above.\n",
        "\n",
        "Keep in mind this dataset has data from all states in the US. To use the functions above, we must filter out all the other states and counties."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-QteE6vnUSA"
      },
      "source": [
        "# Saving the names of the boroughs of NYC (as they appear in the dataset) in an array.\n",
        "FiveBoroughs = [\"Bronx\", \"Kings\", \"New York\", \"Queens\", \"Richmond\"]\n",
        "# Filtering out all the other states\n",
        "epa = epa_df.loc[epa_df.State == 'NY']\n",
        "# Filtering out all the counties except the five boroughs of NYC\n",
        "epa = epa[epa[\"County\"].isin(FiveBoroughs)]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFMiolevnUSD"
      },
      "source": [
        "We should now determine the geographical units of the original dataset. We could either do this by manually looking at the dataset or using `fips_check` function we defined earlier. The results ensure us we do not have any FIPS code longer than 11 digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCfhBEs4nUSD",
        "outputId": "9779f2e3-7dd3-4f2a-c600-cda3eb8b2b67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fips_check(epa, 'Tract')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsBY5iv8nUSH"
      },
      "source": [
        "We can now use the `fips_11()` function and save it in a new dataframe. As described above, this function will ensures we only get the columns of interest in our final dataset. However, in this case we need to remove the previous geogrpahical unit column, `Tract`. `fips_11()` creates a column, _CT_ which serves as the new geographical unit column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbm28nqanUSI"
      },
      "source": [
        "epa_11 = fips_11(epa, ['Tract', 'Total Respiratory (hazard quotient)'])\n",
        "epa_11.drop(columns='Tract', inplace=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlp6w7MonUSL"
      },
      "source": [
        "## social Vulnerability Index (SVI)\n",
        "\n",
        "The SVI is developed by CDC (https://svi.cdc.gov/data-and-tools-download.html) to evaluate community risk to hazardous events.\" Several factors, including poverty, lack of access to transportation, and crowded housing may weaken a community’s ability to prevent human suffering and financial loss in a disaster. These factors are known as social vulnerability\" \n",
        "\n",
        "Centers for Disease Control and Prevention/ Agency for Toxic Substances and Disease Registry/ Geospatial Research, Analysis, and Services Program. Social Vulnerability Index 2016 Database New York. data-and-tools-download.html. Accessed on 2018.\n",
        "\n",
        "### Reading the dataset\n",
        "Similar to EPA's dataset, we read this dataset via direct URL from CDC's website."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr7lBT8ZnUSL"
      },
      "source": [
        "url_svi = \"https://svi.cdc.gov/Documents/Data/2016_SVI_Data/CSV/States/NewYork.csv\"\n",
        "svi_df = pd.read_csv(url_svi)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6y0ziqUnUSO"
      },
      "source": [
        "### Preprocessing the dataset\n",
        "This dataset contains all counties in New York state. We only need the subset of the dataset for the 5 boroughs in NY."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckBpOL20nUSP"
      },
      "source": [
        "FiveBoroughs = ['Bronx', 'Kings', 'New York', 'Queens', 'Richmond']\n",
        "svi = svi_df[svi_df['COUNTY'].isin(FiveBoroughs)]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeTP4tK4nUSS"
      },
      "source": [
        "### Final step\n",
        "We are now ready to use our predefined functions to trim the columns and the FIPS codes. In this case, we are using the `fips_11` function which ensures we are extracting census tract level data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPRIHOADnUSS"
      },
      "source": [
        "svi_11 = fips_11(svi, ['FIPS', 'RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4', 'RPL_THEMES'])\n",
        "svi_11.drop(columns='FIPS', inplace=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drf5vbEAnUSV"
      },
      "source": [
        "## Food Access Research Atlas from Economic Research Service \n",
        "\n",
        "https://www.ers.usda.gov/data-products/food-access-research-atlas/ (Data is from 2015.  Data version downloaded was last updated 5/18/2017. Data was downloaded 2018).\n",
        "\n",
        "This is a measure of food access based on supermarket accessibility\n",
        "This is a binary variable of YES[1]/NO[0] low access to food.  (If an area has low food access, the value will be 1). \n",
        "\n",
        "### Reading the dataset\n",
        "\n",
        "Similar to other datasets, we will be reading the dataset via URL from USDA's website."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKsbJdKTnUSW"
      },
      "source": [
        "url_food = \"https://www.ers.usda.gov/webdocs/DataFiles/80591/DataDownload2015.xlsx?v=0\"\n",
        "food_df = pd.read_excel(url_food, sheet_name='Food Access Research Atlas')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHi68YNCnUSY"
      },
      "source": [
        "### Preprocessing\n",
        "This dataset also contains data elements for all states in the United States. We only want the five boroughs of NYC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd4KZ_zQnUSZ"
      },
      "source": [
        "FiveBoroughs = [\"Bronx\", \"Kings\", \"New York\", \"Queens\", \"Richmond\"]\n",
        "food = food_df.loc[food_df.State == \"New York\"]\n",
        "food = food[food[\"County\"].isin(FiveBoroughs)]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GKHVXhInUSa"
      },
      "source": [
        "### Final step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45_QPyE_nUSb"
      },
      "source": [
        "food_11 = fips_11(food, [\"CensusTract\", \"LAhalfand10\"])\n",
        "food_11.drop(columns='CensusTract', inplace=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPmsZgPtnUSd"
      },
      "source": [
        "## NYC Open \n",
        "\n",
        "We used _NYC Open Data_ (https://opendata.cityofnewyork.us/), to access several datasets for many of the variables used in VACCINe.\n",
        "\n",
        "- Trees\n",
        "- Bus Stop \n",
        "- Subway Stops \n",
        "- Crime Data \n",
        "\n",
        "We downloaded a local copy of each dataset and converted pairs of latitude and longitude to FIPS code using the function defined above. We broke up the larger datasets into smaller subsets before conversion, to prevent potential API timeouts. In this walkthrough, we dedicate a section to using the `geo_to_fips` function. However, for the remainder of the walkthrough, we will use the already converted and local version of the datasets.\n",
        "\n",
        "\n",
        "## Trees\n",
        "\n",
        "This measure is a count of alive trees (according to NYC Open dataset) in each geographical area in 2015. \n",
        "\n",
        "https://data.cityofnewyork.us/Environment/2015-Street-Tree-Census-Tree-Data/uvpi-gqnh\n",
        "\n",
        "### Reading the dataset\n",
        "We have uploaded the converted dataset on our GitHub repository, however, you may download the original dataset from the link above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJTE2sJ_nUSe"
      },
      "source": [
        "url_tree = \"https://github.com/SajjadAbedian/SDH_variables/blob/master/treeLocation.zip?raw=true\"\n",
        "r = requests.get(url_tree)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "tree_df = pd.read_csv(z.open('treeLocation.csv')) "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM_JmATwnUSh"
      },
      "source": [
        "### Preprocessing\n",
        "This dataset is a subset of the original data. We only selected trees with the status of _alive_. At this step, we only need to find the count of trees in each geographical unit. We use the _Census_Tract_ column to aggregate the rows. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn-q7qkLnUSh"
      },
      "source": [
        "tree = fips_11(tree_df, ['fips', 'tree_id'])\n",
        "tree_11 = pd.value_counts(tree['CT']).to_frame('tree_count')\n",
        "tree_11['CT'] = tree_11.index"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsoLhHFhnUSk"
      },
      "source": [
        "## Bus shelters\n",
        "\n",
        "This measure is not a count of subway lines but a binary YES[1]/NO[0] value. (Times square has multiple lines and more connected while some other borough stops will have only one subway line).\n",
        "\n",
        "### Source\n",
        "https://data.cityofnewyork.us/Transportation/Bus-Stop-Shelters/qafz-7myz \n",
        "The raw dataset has latitude and longitude coordinates along with a few other data points for each bus shelter in NYC.\n",
        "\n",
        "For our purpose, we need to create a binary indicator for each geographical unit to identify areas with or without bus shelters. Another way of manipulating the raw dataset would be to count the number of bus shelters in each area. (Please see Tree dataset).\n",
        "\n",
        "### Reading Raw Dataset\n",
        "**NOTE** We are also reading a preprocessed file called `bus_df` which has the latitude and longitude pairs converted to FIPS code using the `geo_to_fips()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ct-PtsbnUSl"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/SajjadAbedian/SDH_variables/master/bustopLocation.csv\"\n",
        "bus_df = pd.read_csv(url)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hynmd1UgnUSn"
      },
      "source": [
        "### Creating Binary Indicators\n",
        "In this step we first count the number of bus shelters in each geographical unit. As mentioned earlier, you could stop and use this data element as one of your variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEMcB-iXnUSn"
      },
      "source": [
        "bus = fips_11(bus_df, ['FIPS', 'Location'])\n",
        "bus_11 = pd.value_counts(bus['CT']).to_frame('BusStopCount')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnOUp-ZGnUSq"
      },
      "source": [
        "The next step is to flag the geographical units with at least one bus shelter with `1` and `0` for geographical units with no bus shelter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBAv14EOnUSq"
      },
      "source": [
        "bus_11['busStopYN'] = np.where(bus_11['BusStopCount'] > 0, 1, 0)\n",
        "bus_11['CT'] = bus_11.index"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH_GWLYAnUSs"
      },
      "source": [
        "## Subway Stations & Subway Entrances \n",
        "\n",
        "To accurately identify geographical units with ACCESS to a subway line we need to look at subway entrances as well as subway stations. We need to combine both variables with an OR logical operator. The subway entrance and subway location are in slightly different census tracts or census block groups.  This happens if there are some subways on the border of a geographical area.  \n",
        "\n",
        "For example, the central park has 6 subway stations but the surrounding census tracts do not appear to have subways.  The combination of the two data sets aims to better portray access to subway stations. We understand this to be a limitation of this variable, especially in smaller geographical areas.\n",
        "\n",
        "This measure is not a count of subway lines but a binary YES[1]/NO[0] value. (Times square has multiple lines and more connected while some other borough stops will have only one subway line).\n",
        "\n",
        "### Data Sources\n",
        "#### Subway Stations\n",
        "https://data.cityofnewyork.us/Transportation/Subway-Stations/arq3-7z49\n",
        "\n",
        "#### Subway Entrance\n",
        "https://data.cityofnewyork.us/Transportation/Subway-Entrances/drex-xx56 \n",
        "\n",
        "### Reading the dataset\n",
        "The raw datasets each have a row per subway line/entrance identified by latitude and longitude. Similar to other datasets, we are using a preprocessed dataset where we already converted the pair of latitudes and longitudes to FIPS codes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcK8mDyLnUSs"
      },
      "source": [
        "url_loc = \"https://raw.githubusercontent.com/SajjadAbedian/SDH_variables/master/subwayLocation.csv\"\n",
        "url_entrance = \"https://raw.githubusercontent.com/SajjadAbedian/SDH_variables/master/subwayEntrance.csv\"\n",
        "\n",
        "subloc_df = pd.read_csv(url_loc)\n",
        "subentrance_df = pd.read_csv(url_entrance)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMrKvfTOnUSt"
      },
      "source": [
        "### Creating Binary Indicators\n",
        "In this step we first count the number of subway lines/entrances in each geographical unit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpQGVvvDnUSu"
      },
      "source": [
        "subloc = fips_11(subloc_df, ['FIPS', 'NAME'])\n",
        "subloc_11 = pd.value_counts(subloc['CT']).to_frame('SubwayCount')\n",
        "subloc_11['station_yn'] = np.where(subloc_11['SubwayCount'] > 0, 1, 0)\n",
        "subloc_11['CT'] = subloc_11.index\n",
        "\n",
        "subentrance = fips_11(subentrance_df, ['FIPS', 'NAME'])\n",
        "subentrance_11 = pd.value_counts(subentrance['CT']).to_frame('EntranceCount')\n",
        "subentrance_11['entrance_yn'] = np.where(subentrance_11['EntranceCount'] > 0, 1, 0)\n",
        "subentrance_11['CT'] = subentrance_11.index"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rckqLRlXnUSv"
      },
      "source": [
        "### Combining the two variables\n",
        "In this step, we need to combine the two variables in one. In other words, we need to create a final binary indicator where we assign `1` when a geographical area has either a subway station or a subway entrance and `0` for a geographical area has neither. We then save the results in a new data frame `subway`. We will complete this task at the end once we join all variables together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1QJrwz-nUSv"
      },
      "source": [
        "## Crime\n",
        "### Reading the original dataset\n",
        "This dataset contains all Felonies, Misdemeanors, and Violations. In order to speed up the processing time, we suggest breaking up the dataset based on the crime types. You can further speed up the process by only selecting a few columns for your dataset (i.e. Latitude, Longitude, and LAW_CAT_CD).]\n",
        "\n",
        "In this walkthrough, we briefly go over breaking up the dataset based on the crime type. However, due to the large size of the datasets, we will use a preprocessed dataset which contains converted latitudes and longitudes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KsvefkknUSw"
      },
      "source": [
        "# DO NOT RUN\n",
        "# Steps below detail breaking up the crime dataset to smaller subsets according to the crime type\n",
        "crime = pd.read_csv('NYPD_Complaint_Data_Historic.csv')\n",
        "crime_felony = crime[crime['LAW_CAT_CD'] == 'FELONY']\n",
        "crime_misdemeanor = crime[crime['LAW_CAT_CD'] == 'MISDEMEANOR']\n",
        "crime_violation = crime[crime['LAW_CAT_CD'] == 'VIOLATION']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPGivvRDnUSx"
      },
      "source": [
        "## Felony\n",
        "### Reading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB_bQTtFnUSy"
      },
      "source": [
        "url_felony = \"https://raw.githubusercontent.com/SajjadAbedian/SDH_variables/master/crimeLocationFelony.csv\"\n",
        "felony_df= pd.read_csv(url_felony)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiDi-NXHnUSz"
      },
      "source": [
        "### Preprocessing\n",
        "Similar to the _tree_ dataset we need to get a count of felonies in each geographical unit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svxpMInynUS0"
      },
      "source": [
        "felony = fips_11(felony_df, ['FIPS', 'LAW_CAT_CD'])\n",
        "felony_11 = pd.value_counts(felony['CT']).to_frame('felony_count')\n",
        "felony_11['felony_count'].fillna(0, inplace = True)\n",
        "felony_11['CT'] = felony_11.index"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfXDr0KBnUS1"
      },
      "source": [
        "## Violation\n",
        "### Reading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUyaFOsqnUS1"
      },
      "source": [
        "url_violation = \"https://raw.githubusercontent.com/SajjadAbedian/SDH_variables/master/crimeLocationViolation.csv\"\n",
        "violation_df= pd.read_csv(url_violation)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xlD-w2NnUS3"
      },
      "source": [
        "### Preprocessing\n",
        "Similar to the _tree_ dataset we need to get a count of violations in each geographical unit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnJCmgyunUS3"
      },
      "source": [
        "violation = fips_11(violation_df, ['FIPS', 'LAW_CAT_CD'])\n",
        "violation_11 = pd.value_counts(violation['CT']).to_frame('violation_count')\n",
        "violation_11['violation_count'].fillna(0, inplace = True)\n",
        "violation_11['CT'] = violation_11.index"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLyqDvOCnUS7"
      },
      "source": [
        "## Misdemeanor\n",
        "### Reading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXvzDaoAnUS7"
      },
      "source": [
        "url_misdemeanor = \"https://raw.githubusercontent.com/SajjadAbedian/SDH_variables/master/crimeLocationMisdemeanor.csv\"\n",
        "misdemeanor_df= pd.read_csv(url_misdemeanor)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OJBd00dnUS8"
      },
      "source": [
        "### Preprocessing\n",
        "Similar to the _tree_ dataset we need to get a count of misdemeanors in each geographical unit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HLrjhPgnUS9"
      },
      "source": [
        "misdemeanor = fips_11(misdemeanor_df, ['FIPS', 'LAW_CAT_CD'])\n",
        "misdemeanor_11 = pd.value_counts(misdemeanor['CT']).to_frame('misdemeanor_count')\n",
        "misdemeanor_11['misdemeanor_count'].fillna(0, inplace = True)\n",
        "misdemeanor_11['CT'] = misdemeanor_11.index"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OARPMgA5nUS_"
      },
      "source": [
        "## Putting it all together\n",
        "We now have processed all datasets from different sources. We can now combine them all in one dataset. We start with the dataset that has **ALL** the FIPS code (in this case ACS, `dfCT`). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amE9QR39nUTA"
      },
      "source": [
        "VACCINe = dfCT\n",
        "VACCINe = join_to_vaccine(VACCINe, 'FIPS', epa_11, 'CT')\n",
        "VACCINe = join_to_vaccine(VACCINe, 'FIPS', svi_11, 'CT')\n",
        "VACCINe = join_to_vaccine(VACCINe, 'FIPS', food_11, 'CT')\n",
        "VACCINe = join_to_vaccine(VACCINe, 'FIPS', tree_11, 'CT')\n",
        "VACCINe = join_to_vaccine(VACCINe, 'FIPS', bus_11, 'CT')\n",
        "VACCINe = join_to_vaccine(VACCINe, 'FIPS', subloc_11, 'CT')\n",
        "VACCINe = join_to_vaccine(VACCINe, 'FIPS', subentrance_11, 'CT')\n",
        "VACCINe = join_to_vaccine(VACCINe, 'FIPS', felony_11, 'CT')\n",
        "VACCINe = join_to_vaccine(VACCINe, 'FIPS', violation_11, 'CT')\n",
        "VACCINe = join_to_vaccine(VACCINe, 'FIPS', misdemeanor_11, 'CT')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x1m71tQnUTC"
      },
      "source": [
        "## Creating derived variables\n",
        "In this section, we use total population of each FIPS code (extracted from ACS dataset) and crime dataset to find the percentage of crime per 1000 people in each geographical unit. We then add these additional variables to the main dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-LWYFFPnUTC"
      },
      "source": [
        "VACCINe['felonyPer1000PeoplePercent'] = (VACCINe.felony_count * 1000) / (dfCT.totalPopulation)\n",
        "VACCINe['violationPer1000PeoplePercent'] = (VACCINe.violation_count * 1000) / (dfCT.totalPopulation)\n",
        "VACCINe['misdemeanorPer1000PeoplePercent'] = (VACCINe.misdemeanor_count * 1000) / (dfCT.totalPopulation)\n",
        "VACCINe['subway_yn'] = np.where(np.logical_or(VACCINe['station_yn'] == 1, VACCINe['entrance_yn'] == 1), 1,0)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yr54G5XnUTF"
      },
      "source": [
        "## Data cleanup\n",
        "Before exporting the dataset, we need to make sure we do a series of data cleanups. It includes removing values that are out of range. For example, replacing _-666666666_ from ACS dataset and _-999_ from SVI dataset with _-666_. This will help to unify all numbers that are flagged as out of range. We will also fill all NULL values with _-666_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4Zfc5QgnUTF"
      },
      "source": [
        "#remove values that are -666 from census tract data (low or zero populations)\n",
        "VACCINe['medianHouseIncome'] = np.where(VACCINe['medianHouseIncome']== -666666666.0, -666, VACCINe['medianHouseIncome'])\n",
        "VACCINe['GINI'] = np.where(VACCINe['GINI']== -666666666.0, -666, VACCINe['GINI'])\n",
        "# remove -999 values from SVI (low or zero populations)\n",
        "VACCINe['RPL_THEME1'] = np.where(VACCINe['RPL_THEME1']== -999.0000, -666, VACCINe['RPL_THEME1'])\n",
        "VACCINe['RPL_THEME2'] = np.where(VACCINe['RPL_THEME2']== -999.0000, -666, VACCINe['RPL_THEME2'])\n",
        "VACCINe['RPL_THEME3'] = np.where(VACCINe['RPL_THEME3']== -999.0000, -666, VACCINe['RPL_THEME3'])\n",
        "VACCINe['RPL_THEME4'] = np.where(VACCINe['RPL_THEME4']== -999.0000, -666, VACCINe['RPL_THEME4'])\n",
        "VACCINe['RPL_THEMES'] = np.where(VACCINe['RPL_THEMES']== -999.0000, -666, VACCINe['RPL_THEMES'])\n",
        "# remove NaN values (from dividing by a population of zero )\n",
        "VACCINe = VACCINe.fillna(value = -666)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ4xe6MjnUTH"
      },
      "source": [
        "## Exporting the final dataset\n",
        "We can now confidently export the dataset into a .CSV file. It is good practice to inspect the dataset before exporting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1FJ7XN_nUTH",
        "outputId": "3ec5c084-8a5d-46e4-ad50-354d08baf400",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(VACCINe.columns.values)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['FIPS',\n",
              " 'foriegnBornPercent',\n",
              " 'woInsurance18_64Percent',\n",
              " 'medicaid18_64Percent',\n",
              " 'medicaid_Medicare18_64Percent',\n",
              " 'no_school_to_12_grade_no_diplomaPercent',\n",
              " 'highSchool_GEDPercent',\n",
              " 'unemployedPercent',\n",
              " 'medianHouseIncome',\n",
              " 'belowPovertyPercent',\n",
              " 'GINI',\n",
              " 'totalPopulation',\n",
              " 'Total Respiratory (hazard quotient)',\n",
              " 'RPL_THEME1',\n",
              " 'RPL_THEME2',\n",
              " 'RPL_THEME3',\n",
              " 'RPL_THEME4',\n",
              " 'RPL_THEMES',\n",
              " 'LAhalfand10',\n",
              " 'tree_count',\n",
              " 'BusStopCount',\n",
              " 'busStopYN',\n",
              " 'SubwayCount',\n",
              " 'station_yn',\n",
              " 'EntranceCount',\n",
              " 'entrance_yn',\n",
              " 'felony_count',\n",
              " 'violation_count',\n",
              " 'misdemeanor_count',\n",
              " 'felonyPer1000PeoplePercent',\n",
              " 'violationPer1000PeoplePercent',\n",
              " 'misdemeanorPer1000PeoplePercent',\n",
              " 'subway_yn']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX_EYX5nnUTK"
      },
      "source": [
        "VACCINe.to_csv('VACCINe_' + str(fips_check(VACCINe, 'FIPS')) + '_digits_fips_'+ str(date.today()) + '.csv')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNYlPLuynUTM",
        "outputId": "ac7e436f-c52f-44db-989e-65189bd567e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "files.download('VACCINe_' + str(fips_check(VACCINe, 'FIPS')) + '_digits_fips_'+ str(date.today()) + '.csv')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_807fd08b-8ebe-4e48-8710-e26193c3e073\", \"VACCINe_11_digits_fips_2020-11-13.csv\", 764925)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}